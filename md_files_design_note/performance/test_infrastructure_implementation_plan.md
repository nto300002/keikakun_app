# ãƒ†ã‚¹ãƒˆã‚¤ãƒ³ãƒ•ãƒ©æ”¹å–„ å®Ÿè£…è¨ˆç”»ï¼ˆOption 2ï¼‰

**è¨ˆç”»ç­–å®šæ—¥**: 2026-02-09
**å®Ÿè£…æœŸé–“**: 2é€±é–“ï¼ˆ10å–¶æ¥­æ—¥ï¼‰
**ç›®æ¨™**: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œå¯èƒ½åŒ–ï¼ˆ100äº‹æ¥­æ‰€è¦æ¨¡ï¼‰

---

## ğŸ“‹ å®Ÿè£…æ¦‚è¦

### ç›®çš„
1. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆé€Ÿåº¦ã‚’12å€æ”¹å–„ï¼ˆ60åˆ† â†’ 5åˆ†ï¼‰
2. 100äº‹æ¥­æ‰€è¦æ¨¡ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œå¯èƒ½ã«
3. CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«çµ±åˆå¯èƒ½ãªãƒ†ã‚¹ãƒˆç’°å¢ƒæ§‹ç¯‰

### ã‚¹ã‚³ãƒ¼ãƒ—
- âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã®æœ€é©åŒ–ï¼ˆãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆã€ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆï¼‰
- âœ… ãƒ†ã‚¹ãƒˆç”¨DBç’°å¢ƒã®å¼·åŒ–ï¼ˆå°‚ç”¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã€æ¥ç¶šãƒ—ãƒ¼ãƒ«æ‹¡å¤§ï¼‰
- âœ… æ®µéšçš„ãƒ†ã‚¹ãƒˆã®å®Ÿè£…ï¼ˆsmall/medium/largeï¼‰
- âŒ CI/CDçµ±åˆï¼ˆä»Šå›ã®ã‚¹ã‚³ãƒ¼ãƒ—å¤–ã€å°†æ¥æ¤œè¨ï¼‰
- âŒ ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼ˆä»Šå›ã®ã‚¹ã‚³ãƒ¼ãƒ—å¤–ã€å°†æ¥æ¤œè¨ï¼‰

---

## ğŸ“… å®Ÿè£…ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

### Week 1: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã®æœ€é©åŒ–ï¼ˆ5æ—¥é–“ï¼‰

```
Day 1-2: ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆå®Ÿè£…
Day 3-4: ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆæ©Ÿèƒ½å®Ÿè£…
Day 5:   Week 1 çµ±åˆãƒ†ã‚¹ãƒˆãƒ»æ¤œè¨¼
```

### Week 2: ãƒ†ã‚¹ãƒˆç”¨DBç’°å¢ƒã®å¼·åŒ–ï¼ˆ5æ—¥é–“ï¼‰

```
Day 6-7: å°‚ç”¨DBã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹æ§‹ç¯‰
Day 8-9: æ®µéšçš„ãƒ†ã‚¹ãƒˆã®å®Ÿè£…
Day 10:  Week 2 çµ±åˆãƒ†ã‚¹ãƒˆãƒ»æœ€çµ‚æ¤œè¨¼
```

---

## ğŸ¯ Week 1: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã®æœ€é©åŒ–

### Day 1-2: ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆå®Ÿè£…

#### ã‚¿ã‚¹ã‚¯1.1: ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆç”¨ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã®ä½œæˆ

**ãƒ•ã‚¡ã‚¤ãƒ«**: `tests/conftest.py`ï¼ˆã¾ãŸã¯æ–°è¦ `tests/performance/bulk_factories.py`ï¼‰

**å®Ÿè£…å†…å®¹**:

```python
"""
ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆç”¨ã®ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ˜ãƒ«ãƒ‘ãƒ¼
"""
from typing import List, Dict
from uuid import UUID
from sqlalchemy.ext.asyncio import AsyncSession
from app.models import Office, Staff, WelfareRecipient, IndividualSupportPlan, SupportPlanCycle


async def bulk_create_offices(
    db: AsyncSession,
    count: int,
    batch_size: int = 100
) -> List[Office]:
    """
    äº‹æ¥­æ‰€ã‚’ä¸€æ‹¬ä½œæˆ

    Args:
        db: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚»ãƒƒã‚·ãƒ§ãƒ³
        count: ä½œæˆã™ã‚‹äº‹æ¥­æ‰€æ•°
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º

    Returns:
        List[Office]: ä½œæˆã—ãŸäº‹æ¥­æ‰€ã®ãƒªã‚¹ãƒˆ
    """
    offices = []
    for i in range(count):
        office = Office(
            name=f"ãƒ†ã‚¹ãƒˆäº‹æ¥­æ‰€{i:04d}",
            address=f"æ±äº¬éƒ½ãƒ†ã‚¹ãƒˆåŒº{i}",
            phone_number=f"03-0000-{i:04d}",
            is_test_data=True
        )
        offices.append(office)

    # ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆï¼ˆbatch_sizeãšã¤ï¼‰
    for i in range(0, len(offices), batch_size):
        batch = offices[i:i + batch_size]
        db.add_all(batch)
        await db.flush()

    await db.commit()

    # IDã‚’å–å¾—ã™ã‚‹ãŸã‚ã«refresh
    for office in offices:
        await db.refresh(office)

    return offices


async def bulk_create_staffs(
    db: AsyncSession,
    offices: List[Office],
    count_per_office: int,
    batch_size: int = 100
) -> Dict[UUID, List[Staff]]:
    """
    ã‚¹ã‚¿ãƒƒãƒ•ã‚’ä¸€æ‹¬ä½œæˆ

    Args:
        db: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚»ãƒƒã‚·ãƒ§ãƒ³
        offices: äº‹æ¥­æ‰€ãƒªã‚¹ãƒˆ
        count_per_office: äº‹æ¥­æ‰€ã‚ãŸã‚Šã®ã‚¹ã‚¿ãƒƒãƒ•æ•°
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º

    Returns:
        Dict[UUID, List[Staff]]: {office_id: [staff, ...]}
    """
    staffs = []
    staffs_by_office = {office.id: [] for office in offices}

    for office in offices:
        for i in range(count_per_office):
            staff = Staff(
                office_id=office.id,
                first_name=f"ã‚¹ã‚¿ãƒƒãƒ•{i:03d}",
                last_name=f"äº‹æ¥­æ‰€{offices.index(office):04d}",
                full_name=f"äº‹æ¥­æ‰€{offices.index(office):04d} ã‚¹ã‚¿ãƒƒãƒ•{i:03d}",
                email=f"staff_{office.id}_{i}@example.com",
                hashed_password="$2b$12$dummy_hash_for_testing",
                role="employee",
                is_test_data=True,
                notification_preferences={
                    "in_app_notification": True,
                    "email_notification": True,
                    "system_notification": False,
                    "email_threshold_days": 30,
                    "push_threshold_days": 10
                }
            )
            staffs.append(staff)
            staffs_by_office[office.id].append(staff)

    # ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
    for i in range(0, len(staffs), batch_size):
        batch = staffs[i:i + batch_size]
        db.add_all(batch)
        await db.flush()

    await db.commit()

    # IDã‚’å–å¾—ã™ã‚‹ãŸã‚ã«refresh
    for staff in staffs:
        await db.refresh(staff)

    return staffs_by_office


async def bulk_create_welfare_recipients(
    db: AsyncSession,
    offices: List[Office],
    count_per_office: int,
    batch_size: int = 100
) -> Dict[UUID, List[WelfareRecipient]]:
    """
    åˆ©ç”¨è€…ã‚’ä¸€æ‹¬ä½œæˆ

    Args:
        db: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚»ãƒƒã‚·ãƒ§ãƒ³
        offices: äº‹æ¥­æ‰€ãƒªã‚¹ãƒˆ
        count_per_office: äº‹æ¥­æ‰€ã‚ãŸã‚Šã®åˆ©ç”¨è€…æ•°
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º

    Returns:
        Dict[UUID, List[WelfareRecipient]]: {office_id: [recipient, ...]}
    """
    recipients = []
    recipients_by_office = {office.id: [] for office in offices}

    for office in offices:
        for i in range(count_per_office):
            recipient = WelfareRecipient(
                office_id=office.id,
                first_name=f"åˆ©ç”¨è€…{i:03d}",
                last_name=f"äº‹æ¥­æ‰€{offices.index(office):04d}",
                full_name=f"äº‹æ¥­æ‰€{offices.index(office):04d} åˆ©ç”¨è€…{i:03d}",
                date_of_birth="1990-01-01",
                is_test_data=True
            )
            recipients.append(recipient)
            recipients_by_office[office.id].append(recipient)

    # ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
    for i in range(0, len(recipients), batch_size):
        batch = recipients[i:i + batch_size]
        db.add_all(batch)
        await db.flush()

    await db.commit()

    # IDã‚’å–å¾—ã™ã‚‹ãŸã‚ã«refresh
    for recipient in recipients:
        await db.refresh(recipient)

    return recipients_by_office


async def bulk_create_support_plans_and_cycles(
    db: AsyncSession,
    recipients_by_office: Dict[UUID, List[WelfareRecipient]],
    staffs_by_office: Dict[UUID, List[Staff]],
    batch_size: int = 100
) -> tuple[List[IndividualSupportPlan], List[SupportPlanCycle]]:
    """
    å€‹åˆ¥æ”¯æ´è¨ˆç”»ã¨ã‚µã‚¤ã‚¯ãƒ«ã‚’ä¸€æ‹¬ä½œæˆ

    Args:
        db: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚»ãƒƒã‚·ãƒ§ãƒ³
        recipients_by_office: {office_id: [recipient, ...]}
        staffs_by_office: {office_id: [staff, ...]}
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º

    Returns:
        tuple: (plans, cycles)
    """
    from datetime import date, timedelta

    plans = []
    cycles = []

    for office_id, recipients in recipients_by_office.items():
        staffs = staffs_by_office[office_id]
        if not staffs:
            continue

        for recipient in recipients:
            # å€‹åˆ¥æ”¯æ´è¨ˆç”»ä½œæˆ
            plan = IndividualSupportPlan(
                office_id=office_id,
                welfare_recipient_id=recipient.id,
                responsible_staff_id=staffs[0].id,  # æœ€åˆã®ã‚¹ã‚¿ãƒƒãƒ•
                is_test_data=True
            )
            plans.append(plan)

    # è¨ˆç”»ã‚’ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
    for i in range(0, len(plans), batch_size):
        batch = plans[i:i + batch_size]
        db.add_all(batch)
        await db.flush()

    # IDã‚’å–å¾—
    for plan in plans:
        await db.refresh(plan)

    # ã‚µã‚¤ã‚¯ãƒ«ä½œæˆ
    today = date.today()
    for plan in plans:
        # æœŸé™ãŒè¿‘ã„ã‚µã‚¤ã‚¯ãƒ«ã‚’ä½œæˆï¼ˆ30æ—¥ä»¥å†…ï¼‰
        cycle = SupportPlanCycle(
            individual_support_plan_id=plan.id,
            start_date=today - timedelta(days=335),  # 11ãƒ¶æœˆå‰é–‹å§‹
            end_date=today + timedelta(days=25),     # 25æ—¥å¾Œçµ‚äº†
            status="active",
            is_test_data=True
        )
        cycles.append(cycle)

    # ã‚µã‚¤ã‚¯ãƒ«ã‚’ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
    for i in range(0, len(cycles), batch_size):
        batch = cycles[i:i + batch_size]
        db.add_all(batch)
        await db.flush()

    await db.commit()

    return plans, cycles
```

**æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„**:
```
Before: 5,000ã‚¹ã‚¿ãƒƒãƒ• Ã— 0.2ç§’ = 1,000ç§’ï¼ˆ16åˆ†ï¼‰
After:  5,000ã‚¹ã‚¿ãƒƒãƒ• Ã· 100ãƒãƒƒãƒ Ã— 0.5ç§’ = 25ç§’

æ”¹å–„ç‡: 40å€é«˜é€ŸåŒ–
```

---

#### ã‚¿ã‚¹ã‚¯1.2: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£ã®æ›´æ–°

**ãƒ•ã‚¡ã‚¤ãƒ«**: `tests/performance/test_deadline_notification_performance.py`

**å®Ÿè£…å†…å®¹**:

```python
import pytest_asyncio
from tests.performance.bulk_factories import (
    bulk_create_offices,
    bulk_create_staffs,
    bulk_create_welfare_recipients,
    bulk_create_support_plans_and_cycles
)


@pytest_asyncio.fixture
async def performance_test_data_medium(db_session: AsyncSession):
    """
    ä¸­è¦æ¨¡ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ï¼ˆ100äº‹æ¥­æ‰€ï¼‰

    ãƒ‡ãƒ¼ã‚¿æ§‹æˆ:
    - 100äº‹æ¥­æ‰€
    - 1,000ã‚¹ã‚¿ãƒƒãƒ•ï¼ˆå„äº‹æ¥­æ‰€10åï¼‰
    - 10,000åˆ©ç”¨è€…ï¼ˆå„äº‹æ¥­æ‰€100åï¼‰
    - 10,000å€‹åˆ¥æ”¯æ´è¨ˆç”»
    - 10,000ã‚µã‚¤ã‚¯ãƒ«ï¼ˆæœŸé™30æ—¥ä»¥å†…ï¼‰

    æœŸå¾…ç”Ÿæˆæ™‚é–“: 5åˆ†ä»¥å†…
    """
    print("\nâ³ ä¸­è¦æ¨¡ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆé–‹å§‹ï¼ˆ100äº‹æ¥­æ‰€ã€1,000ã‚¹ã‚¿ãƒƒãƒ•ï¼‰...")

    # Step 1: äº‹æ¥­æ‰€ä½œæˆï¼ˆãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆï¼‰
    offices = await bulk_create_offices(db_session, count=100)
    print(f"âœ… äº‹æ¥­æ‰€ä½œæˆå®Œäº†: {len(offices)}ä»¶")

    # Step 2: ã‚¹ã‚¿ãƒƒãƒ•ä½œæˆï¼ˆãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆï¼‰
    staffs_by_office = await bulk_create_staffs(
        db_session,
        offices=offices,
        count_per_office=10
    )
    total_staffs = sum(len(staffs) for staffs in staffs_by_office.values())
    print(f"âœ… ã‚¹ã‚¿ãƒƒãƒ•ä½œæˆå®Œäº†: {total_staffs}ä»¶")

    # Step 3: åˆ©ç”¨è€…ä½œæˆï¼ˆãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆï¼‰
    recipients_by_office = await bulk_create_welfare_recipients(
        db_session,
        offices=offices,
        count_per_office=100
    )
    total_recipients = sum(len(recs) for recs in recipients_by_office.values())
    print(f"âœ… åˆ©ç”¨è€…ä½œæˆå®Œäº†: {total_recipients}ä»¶")

    # Step 4: å€‹åˆ¥æ”¯æ´è¨ˆç”»ã¨ã‚µã‚¤ã‚¯ãƒ«ä½œæˆï¼ˆãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆï¼‰
    plans, cycles = await bulk_create_support_plans_and_cycles(
        db_session,
        recipients_by_office=recipients_by_office,
        staffs_by_office=staffs_by_office
    )
    print(f"âœ… å€‹åˆ¥æ”¯æ´è¨ˆç”»ä½œæˆå®Œäº†: {len(plans)}ä»¶")
    print(f"âœ… ã‚µã‚¤ã‚¯ãƒ«ä½œæˆå®Œäº†: {len(cycles)}ä»¶")

    print("âœ… ä¸­è¦æ¨¡ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†")

    return {
        "offices": offices,
        "staffs_by_office": staffs_by_office,
        "recipients_by_office": recipients_by_office,
        "plans": plans,
        "cycles": cycles,
        "expected_emails": total_staffs  # å…¨ã‚¹ã‚¿ãƒƒãƒ•ã«ãƒ¡ãƒ¼ãƒ«é€ä¿¡
    }
```

---

### Day 3-4: ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆæ©Ÿèƒ½å®Ÿè£…

#### ã‚¿ã‚¹ã‚¯2.1: ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆç®¡ç†ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã®ä½œæˆ

**ãƒ•ã‚¡ã‚¤ãƒ«**: `tests/performance/snapshot_manager.py`ï¼ˆæ–°è¦ä½œæˆï¼‰

**å®Ÿè£…å†…å®¹**:

```python
"""
ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆç®¡ç†

ç›®çš„: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’äº‹å‰ç”Ÿæˆã—ã¦ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆåŒ–ã—ã€ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚ã«é«˜é€Ÿãƒªã‚¹ãƒˆã‚¢
"""
import subprocess
import os
from pathlib import Path
from datetime import datetime
from typing import Optional
import logging

logger = logging.getLogger(__name__)

SNAPSHOT_DIR = Path(__file__).parent / "snapshots"
SNAPSHOT_DIR.mkdir(exist_ok=True)


class SnapshotManager:
    """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆç®¡ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self, db_url: str):
        """
        Args:
            db_url: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šURLï¼ˆä¾‹: postgresql://user:pass@host:port/dbnameï¼‰
        """
        self.db_url = db_url
        self._parse_db_url()

    def _parse_db_url(self):
        """DB URLã‚’ãƒ‘ãƒ¼ã‚¹"""
        # postgresql://user:pass@host:port/dbname
        from urllib.parse import urlparse
        parsed = urlparse(self.db_url)

        self.db_host = parsed.hostname
        self.db_port = parsed.port or 5432
        self.db_name = parsed.path.lstrip('/')
        self.db_user = parsed.username
        self.db_password = parsed.password

    def create_snapshot(self, snapshot_name: str) -> Path:
        """
        ç¾åœ¨ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’ä½œæˆ

        Args:
            snapshot_name: ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆåï¼ˆä¾‹: "medium_100_offices"ï¼‰

        Returns:
            Path: ä½œæˆã•ã‚ŒãŸã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        snapshot_file = SNAPSHOT_DIR / f"{snapshot_name}_{timestamp}.sql"

        logger.info(f"Creating snapshot: {snapshot_file}")

        # pg_dumpã§ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆä½œæˆ
        env = os.environ.copy()
        env['PGPASSWORD'] = self.db_password

        cmd = [
            "pg_dump",
            "-h", self.db_host,
            "-p", str(self.db_port),
            "-U", self.db_user,
            "-d", self.db_name,
            "-f", str(snapshot_file),
            "--clean",  # DROPæ–‡ã‚’å«ã‚ã‚‹
            "--if-exists",  # IF EXISTSã‚’è¿½åŠ 
            "--no-owner",  # ã‚ªãƒ¼ãƒŠãƒ¼æƒ…å ±ã‚’å«ã‚ãªã„
            "--no-privileges"  # æ¨©é™æƒ…å ±ã‚’å«ã‚ãªã„
        ]

        result = subprocess.run(cmd, env=env, capture_output=True, text=True)

        if result.returncode != 0:
            logger.error(f"Snapshot creation failed: {result.stderr}")
            raise RuntimeError(f"Failed to create snapshot: {result.stderr}")

        logger.info(f"âœ… Snapshot created: {snapshot_file} ({snapshot_file.stat().st_size / 1024 / 1024:.2f} MB)")

        # æœ€æ–°ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã¸ã®ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã‚’ä½œæˆ
        latest_link = SNAPSHOT_DIR / f"{snapshot_name}_latest.sql"
        if latest_link.exists() or latest_link.is_symlink():
            latest_link.unlink()
        latest_link.symlink_to(snapshot_file.name)

        return snapshot_file

    def restore_snapshot(self, snapshot_name: str, use_latest: bool = True) -> None:
        """
        ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‹ã‚‰ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚¹ãƒˆã‚¢

        Args:
            snapshot_name: ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå
            use_latest: Trueã®å ´åˆã€æœ€æ–°ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’ä½¿ç”¨

        Raises:
            FileNotFoundError: ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
        """
        if use_latest:
            snapshot_file = SNAPSHOT_DIR / f"{snapshot_name}_latest.sql"
            if not snapshot_file.exists():
                raise FileNotFoundError(f"Snapshot not found: {snapshot_file}")
        else:
            # ç‰¹å®šã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
            snapshots = list(SNAPSHOT_DIR.glob(f"{snapshot_name}_*.sql"))
            if not snapshots:
                raise FileNotFoundError(f"No snapshots found for: {snapshot_name}")
            snapshot_file = max(snapshots, key=lambda p: p.stat().st_mtime)

        logger.info(f"Restoring snapshot: {snapshot_file}")

        # psqlã§ãƒªã‚¹ãƒˆã‚¢
        env = os.environ.copy()
        env['PGPASSWORD'] = self.db_password

        cmd = [
            "psql",
            "-h", self.db_host,
            "-p", str(self.db_port),
            "-U", self.db_user,
            "-d", self.db_name,
            "-f", str(snapshot_file),
            "-q"  # é™ã‹ã«å®Ÿè¡Œ
        ]

        result = subprocess.run(cmd, env=env, capture_output=True, text=True)

        if result.returncode != 0:
            logger.error(f"Snapshot restore failed: {result.stderr}")
            raise RuntimeError(f"Failed to restore snapshot: {result.stderr}")

        logger.info(f"âœ… Snapshot restored: {snapshot_file}")

    def list_snapshots(self) -> list[Path]:
        """
        åˆ©ç”¨å¯èƒ½ãªã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã®ãƒªã‚¹ãƒˆã‚’å–å¾—

        Returns:
            list[Path]: ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
        """
        snapshots = list(SNAPSHOT_DIR.glob("*.sql"))
        # ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã‚’é™¤å¤–
        snapshots = [s for s in snapshots if not s.is_symlink()]
        snapshots.sort(key=lambda p: p.stat().st_mtime, reverse=True)
        return snapshots

    def delete_old_snapshots(self, snapshot_name: str, keep_count: int = 3) -> None:
        """
        å¤ã„ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’å‰Šé™¤

        Args:
            snapshot_name: ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå
            keep_count: ä¿æŒã™ã‚‹ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆæ•°
        """
        snapshots = list(SNAPSHOT_DIR.glob(f"{snapshot_name}_*.sql"))
        snapshots = [s for s in snapshots if not s.is_symlink()]
        snapshots.sort(key=lambda p: p.stat().st_mtime, reverse=True)

        # keep_countä»¥é™ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        for snapshot in snapshots[keep_count:]:
            logger.info(f"Deleting old snapshot: {snapshot}")
            snapshot.unlink()
```

---

#### ã‚¿ã‚¹ã‚¯2.2: ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä½œæˆ

**ãƒ•ã‚¡ã‚¤ãƒ«**: `tests/performance/generate_snapshots.py`ï¼ˆæ–°è¦ä½œæˆï¼‰

**å®Ÿè£…å†…å®¹**:

```python
"""
ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å®Ÿè¡Œæ–¹æ³•:
    docker exec keikakun_app-backend-1 python -m tests.performance.generate_snapshots
"""
import asyncio
import os
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
from sqlalchemy.orm import sessionmaker

from tests.performance.snapshot_manager import SnapshotManager
from tests.performance.bulk_factories import (
    bulk_create_offices,
    bulk_create_staffs,
    bulk_create_welfare_recipients,
    bulk_create_support_plans_and_cycles
)


async def generate_medium_snapshot(db_url: str):
    """
    ä¸­è¦æ¨¡ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’ç”Ÿæˆ

    ãƒ‡ãƒ¼ã‚¿æ§‹æˆ:
    - 100äº‹æ¥­æ‰€
    - 1,000ã‚¹ã‚¿ãƒƒãƒ•
    - 10,000åˆ©ç”¨è€…
    - 10,000è¨ˆç”»ãƒ»ã‚µã‚¤ã‚¯ãƒ«
    """
    print("\n" + "=" * 80)
    print("ğŸ“¸ ä¸­è¦æ¨¡ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆç”Ÿæˆé–‹å§‹ï¼ˆ100äº‹æ¥­æ‰€ï¼‰")
    print("=" * 80)

    # DBã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆ
    engine = create_async_engine(db_url, echo=False)
    async_session = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)

    async with async_session() as db:
        # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        print("\nâ³ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­...")

        offices = await bulk_create_offices(db, count=100)
        print(f"âœ… äº‹æ¥­æ‰€: {len(offices)}ä»¶")

        staffs_by_office = await bulk_create_staffs(db, offices, count_per_office=10)
        total_staffs = sum(len(s) for s in staffs_by_office.values())
        print(f"âœ… ã‚¹ã‚¿ãƒƒãƒ•: {total_staffs}ä»¶")

        recipients_by_office = await bulk_create_welfare_recipients(db, offices, count_per_office=100)
        total_recipients = sum(len(r) for r in recipients_by_office.values())
        print(f"âœ… åˆ©ç”¨è€…: {total_recipients}ä»¶")

        plans, cycles = await bulk_create_support_plans_and_cycles(
            db, recipients_by_office, staffs_by_office
        )
        print(f"âœ… è¨ˆç”»: {len(plans)}ä»¶ã€ã‚µã‚¤ã‚¯ãƒ«: {len(cycles)}ä»¶")

    await engine.dispose()

    # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆä½œæˆ
    print("\nâ³ ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆä½œæˆä¸­...")
    snapshot_manager = SnapshotManager(db_url)
    snapshot_file = snapshot_manager.create_snapshot("medium_100_offices")

    print("\n" + "=" * 80)
    print(f"âœ… ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆç”Ÿæˆå®Œäº†: {snapshot_file.name}")
    print("=" * 80)


async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    db_url = os.getenv("TEST_DATABASE_URL")
    if not db_url:
        print("âŒ ERROR: TEST_DATABASE_URL environment variable not set")
        return

    await generate_medium_snapshot(db_url)


if __name__ == "__main__":
    asyncio.run(main())
```

---

#### ã‚¿ã‚¹ã‚¯2.3: ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆåˆ©ç”¨ã®ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£å®Ÿè£…

**ãƒ•ã‚¡ã‚¤ãƒ«**: `tests/performance/test_deadline_notification_performance.py`

**å®Ÿè£…å†…å®¹**:

```python
@pytest_asyncio.fixture
async def performance_test_data_medium_snapshot(db_session: AsyncSession):
    """
    ä¸­è¦æ¨¡ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆåˆ©ç”¨ï¼‰

    ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ãƒªã‚¹ãƒˆã‚¢ã—ã€å­˜åœ¨ã—ãªã„å ´åˆã¯ç”Ÿæˆ
    """
    from tests.performance.snapshot_manager import SnapshotManager

    db_url = os.getenv("TEST_DATABASE_URL")
    snapshot_manager = SnapshotManager(db_url)

    snapshot_name = "medium_100_offices"

    try:
        # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãŒå­˜åœ¨ã™ã‚Œã°ãƒªã‚¹ãƒˆã‚¢
        print(f"\nâ³ ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‹ã‚‰ãƒªã‚¹ãƒˆã‚¢ä¸­: {snapshot_name}")
        snapshot_manager.restore_snapshot(snapshot_name, use_latest=True)
        print("âœ… ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãƒªã‚¹ãƒˆã‚¢å®Œäº†ï¼ˆç´„30ç§’ï¼‰")

        # ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ï¼ˆäº‹æ¥­æ‰€æ•°ã‚’ç¢ºèªï¼‰
        from app.models import Office
        stmt = select(Office).where(Office.is_test_data == True)
        result = await db_session.execute(stmt)
        offices = result.scalars().all()

        if len(offices) < 100:
            raise ValueError(f"Insufficient offices in snapshot: {len(offices)} < 100")

    except (FileNotFoundError, ValueError) as e:
        # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãŒãªã„ã€ã¾ãŸã¯ä¸æ­£ãªå ´åˆã¯ç”Ÿæˆ
        print(f"\nâš ï¸  ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“: {e}")
        print("â³ æ–°è¦ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­...")

        # æ—¢å­˜ã®mediumãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£ã‚’ä½¿ç”¨
        data = await performance_test_data_medium(db_session)
        return data

    # ç”Ÿæˆæ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±ã‚’è¿”ã™
    return {
        "snapshot_restored": True,
        "expected_emails": 1000  # 100äº‹æ¥­æ‰€ Ã— 10ã‚¹ã‚¿ãƒƒãƒ•
    }
```

**æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„**:
```
Before: 100äº‹æ¥­æ‰€ç”Ÿæˆ = 60åˆ†
After:  ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãƒªã‚¹ãƒˆã‚¢ = 30ç§’

æ”¹å–„ç‡: 120å€é«˜é€ŸåŒ–
```

---

### Day 5: Week 1 çµ±åˆãƒ†ã‚¹ãƒˆãƒ»æ¤œè¨¼

#### ã‚¿ã‚¹ã‚¯3.1: ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ

**ãƒ†ã‚¹ãƒˆå†…å®¹**:
```python
@pytest.mark.asyncio
@pytest.mark.performance
async def test_bulk_insert_performance(db_session):
    """
    ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ

    æ¤œè¨¼é …ç›®:
    - 100äº‹æ¥­æ‰€ã‚’5åˆ†ä»¥å†…ã§ç”Ÿæˆ
    - ãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ç¢ºèª
    """
    import time
    start_time = time.time()

    # ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆã§ç”Ÿæˆ
    offices = await bulk_create_offices(db_session, count=100)
    staffs_by_office = await bulk_create_staffs(db_session, offices, count_per_office=10)

    elapsed = time.time() - start_time

    # æ¤œè¨¼
    assert len(offices) == 100
    assert sum(len(s) for s in staffs_by_office.values()) == 1000
    assert elapsed < 300, f"Generation took too long: {elapsed}s > 300s"

    print(f"âœ… 100äº‹æ¥­æ‰€ç”Ÿæˆæ™‚é–“: {elapsed:.1f}ç§’")
```

#### ã‚¿ã‚¹ã‚¯3.2: ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆæ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ

**ãƒ†ã‚¹ãƒˆå†…å®¹**:
```python
@pytest.mark.asyncio
@pytest.mark.performance
async def test_snapshot_restore_performance(db_session):
    """
    ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãƒªã‚¹ãƒˆã‚¢ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ

    æ¤œè¨¼é …ç›®:
    - ãƒªã‚¹ãƒˆã‚¢ãŒ30ç§’ä»¥å†…ã«å®Œäº†
    - ãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ç¢ºèª
    """
    import time
    from tests.performance.snapshot_manager import SnapshotManager

    db_url = os.getenv("TEST_DATABASE_URL")
    snapshot_manager = SnapshotManager(db_url)

    start_time = time.time()

    # ãƒªã‚¹ãƒˆã‚¢
    snapshot_manager.restore_snapshot("medium_100_offices", use_latest=True)

    elapsed = time.time() - start_time

    # æ¤œè¨¼
    assert elapsed < 30, f"Restore took too long: {elapsed}s > 30s"

    print(f"âœ… ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãƒªã‚¹ãƒˆã‚¢æ™‚é–“: {elapsed:.1f}ç§’")
```

---

## ğŸ¯ Week 2: ãƒ†ã‚¹ãƒˆç”¨DBç’°å¢ƒã®å¼·åŒ–

### Day 6-7: å°‚ç”¨DBã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹æ§‹ç¯‰

#### ã‚¿ã‚¹ã‚¯4.1: Docker Composeè¨­å®šã®ä½œæˆ

**ãƒ•ã‚¡ã‚¤ãƒ«**: `docker-compose.test.yml`ï¼ˆæ–°è¦ä½œæˆï¼‰

**å®Ÿè£…å†…å®¹**:

```yaml
version: '3.8'

services:
  # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå°‚ç”¨DBã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
  test-performance-db:
    image: postgres:15-alpine
    container_name: keikakun_test_performance_db
    environment:
      POSTGRES_DB: keikakun_test_performance
      POSTGRES_USER: test_user
      POSTGRES_PASSWORD: test_password
      # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–è¨­å®š
      POSTGRES_INITDB_ARGS: "-E UTF8 --locale=C"
    command:
      - "postgres"
      # æ¥ç¶šè¨­å®š
      - "-c" "max_connections=200"                # é€šå¸¸: 100
      - "-c" "superuser_reserved_connections=3"

      # ãƒ¡ãƒ¢ãƒªè¨­å®š
      - "-c" "shared_buffers=2GB"                 # é€šå¸¸: 128MB
      - "-c" "effective_cache_size=6GB"           # é€šå¸¸: 4GB
      - "-c" "work_mem=50MB"                      # é€šå¸¸: 4MB
      - "-c" "maintenance_work_mem=512MB"         # é€šå¸¸: 64MB

      # WALè¨­å®š
      - "-c" "wal_buffers=16MB"                   # é€šå¸¸: -1 (è‡ªå‹•)
      - "-c" "checkpoint_completion_target=0.9"   # é€šå¸¸: 0.5
      - "-c" "max_wal_size=2GB"                   # é€šå¸¸: 1GB

      # ã‚¯ã‚¨ãƒªãƒ—ãƒ©ãƒ³ãƒŠãƒ¼è¨­å®š
      - "-c" "random_page_cost=1.1"               # é€šå¸¸: 4.0 (SSDæƒ³å®š)
      - "-c" "effective_io_concurrency=200"       # é€šå¸¸: 1

      # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
      - "-c" "statement_timeout=600000"           # 10åˆ†
      - "-c" "idle_in_transaction_session_timeout=3600000"  # 1æ™‚é–“

      # ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
      - "-c" "log_min_duration_statement=1000"    # 1ç§’ä»¥ä¸Šã®ã‚¯ã‚¨ãƒªã‚’ãƒ­ã‚°
      - "-c" "log_line_prefix=%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h "

    ports:
      - "5433:5432"  # ãƒ›ã‚¹ãƒˆã®5433ãƒãƒ¼ãƒˆã«ãƒãƒƒãƒ”ãƒ³ã‚°

    volumes:
      - test_performance_db_data:/var/lib/postgresql/data
      # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒã‚¦ãƒ³ãƒˆ
      - ./tests/performance/snapshots:/snapshots:ro

    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U test_user -d keikakun_test_performance"]
      interval: 10s
      timeout: 5s
      retries: 5

    # ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G

volumes:
  test_performance_db_data:
    driver: local
```

#### ã‚¿ã‚¹ã‚¯4.2: ãƒ†ã‚¹ãƒˆç”¨DBæ¥ç¶šè¨­å®šã®ä½œæˆ

**ãƒ•ã‚¡ã‚¤ãƒ«**: `tests/performance/config.py`ï¼ˆæ–°è¦ä½œæˆï¼‰

**å®Ÿè£…å†…å®¹**:

```python
"""
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆç”¨DBè¨­å®š
"""
import os
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker


def get_performance_test_db_url() -> str:
    """
    ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆç”¨DB URLã‚’å–å¾—

    ç’°å¢ƒå¤‰æ•°ã®å„ªå…ˆé †ä½:
    1. PERFORMANCE_TEST_DATABASE_URLï¼ˆå°‚ç”¨DBï¼‰
    2. TEST_DATABASE_URLï¼ˆé€šå¸¸ãƒ†ã‚¹ãƒˆDBï¼‰
    """
    db_url = os.getenv(
        "PERFORMANCE_TEST_DATABASE_URL",
        os.getenv("TEST_DATABASE_URL")
    )

    if not db_url:
        raise ValueError("Database URL not configured for performance tests")

    return db_url


def create_performance_test_engine():
    """
    ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆç”¨ã®DBã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½œæˆ

    è¨­å®š:
    - æ¥ç¶šãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚º: 20ï¼ˆé€šå¸¸: 10ï¼‰
    - ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼: 30ï¼ˆé€šå¸¸: 10ï¼‰
    - ãƒ—ãƒ¼ãƒ«ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: 60ç§’ï¼ˆé€šå¸¸: 30ç§’ï¼‰
    """
    db_url = get_performance_test_db_url()

    engine = create_async_engine(
        db_url,
        echo=False,  # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã§ã¯ãƒ­ã‚°ç„¡åŠ¹åŒ–
        pool_size=20,
        max_overflow=30,
        pool_timeout=60,
        pool_recycle=3600,  # 1æ™‚é–“
        pool_pre_ping=True,  # æ¥ç¶šãƒã‚§ãƒƒã‚¯
        connect_args={
            "server_settings": {
                "application_name": "performance_test",
                "statement_timeout": "600000"  # 10åˆ†
            }
        }
    )

    return engine


def create_performance_test_session():
    """
    ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆç”¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    """
    engine = create_performance_test_engine()
    async_session = sessionmaker(
        engine,
        class_=AsyncSession,
        expire_on_commit=False,
        autoflush=False  # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã§ã¯æ‰‹å‹•flush
    )
    return async_session
```

---

### Day 8-9: æ®µéšçš„ãƒ†ã‚¹ãƒˆã®å®Ÿè£…

#### ã‚¿ã‚¹ã‚¯5.1: ãƒ†ã‚¹ãƒˆè¦æ¨¡å®šç¾©ã®å®Ÿè£…

**ãƒ•ã‚¡ã‚¤ãƒ«**: `tests/performance/test_scales.py`ï¼ˆæ–°è¦ä½œæˆï¼‰

**å®Ÿè£…å†…å®¹**:

```python
"""
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã®è¦æ¨¡å®šç¾©
"""
from dataclasses import dataclass
from typing import Literal


@dataclass
class TestScale:
    """ãƒ†ã‚¹ãƒˆè¦æ¨¡ã®å®šç¾©"""
    name: str
    offices: int
    staff_per_office: int
    users_per_office: int
    frequency: Literal["every_commit", "daily", "weekly"]
    timeout: int  # ç§’
    description: str


# ãƒ†ã‚¹ãƒˆè¦æ¨¡ã®å®šç¾©
TEST_SCALES = {
    "small": TestScale(
        name="small",
        offices=10,
        staff_per_office=10,
        users_per_office=100,
        frequency="every_commit",
        timeout=600,  # 10åˆ†
        description="å°è¦æ¨¡ãƒ†ã‚¹ãƒˆï¼ˆæ¯commitå®Ÿè¡Œï¼‰"
    ),
    "medium": TestScale(
        name="medium",
        offices=100,
        staff_per_office=10,
        users_per_office=100,
        frequency="daily",
        timeout=1800,  # 30åˆ†
        description="ä¸­è¦æ¨¡ãƒ†ã‚¹ãƒˆï¼ˆæ¯æ—¥å®Ÿè¡Œï¼‰"
    ),
    "large": TestScale(
        name="large",
        offices=500,
        staff_per_office=10,
        users_per_office=100,
        frequency="weekly",
        timeout=3600,  # 60åˆ†
        description="å¤§è¦æ¨¡ãƒ†ã‚¹ãƒˆï¼ˆæ¯é€±å®Ÿè¡Œï¼‰"
    )
}


def get_test_scale(scale_name: str) -> TestScale:
    """
    ãƒ†ã‚¹ãƒˆè¦æ¨¡ã‚’å–å¾—

    Args:
        scale_name: è¦æ¨¡åï¼ˆsmall/medium/largeï¼‰

    Returns:
        TestScale: ãƒ†ã‚¹ãƒˆè¦æ¨¡å®šç¾©

    Raises:
        ValueError: ä¸æ­£ãªè¦æ¨¡å
    """
    if scale_name not in TEST_SCALES:
        raise ValueError(f"Invalid test scale: {scale_name}. Must be one of {list(TEST_SCALES.keys())}")

    return TEST_SCALES[scale_name]
```

#### ã‚¿ã‚¹ã‚¯5.2: ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆã®å®Ÿè£…

**ãƒ•ã‚¡ã‚¤ãƒ«**: `tests/performance/test_deadline_notification_performance.py`

**å®Ÿè£…å†…å®¹**:

```python
import pytest
import os
from tests.performance.test_scales import get_test_scale, TEST_SCALES


@pytest.mark.asyncio
@pytest.mark.performance
@pytest.mark.parametrize("scale_name", ["small", "medium", "large"])
async def test_deadline_notification_scalability(
    db_session: AsyncSession,
    scale_name: str
):
    """
    æœŸé™é€šçŸ¥ãƒãƒƒãƒã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ

    ãƒ†ã‚¹ãƒˆè¦æ¨¡:
    - small: 10äº‹æ¥­æ‰€ï¼ˆæ¯commitï¼‰
    - medium: 100äº‹æ¥­æ‰€ï¼ˆæ¯æ—¥ï¼‰
    - large: 500äº‹æ¥­æ‰€ï¼ˆæ¯é€±ï¼‰

    ç’°å¢ƒå¤‰æ•°ã§ãƒ†ã‚¹ãƒˆè¦æ¨¡ã‚’åˆ¶å¾¡:
    - PERF_TEST_SCALE=small: å°è¦æ¨¡ã®ã¿å®Ÿè¡Œï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
    - PERF_TEST_SCALE=medium: ä¸­è¦æ¨¡ã¾ã§å®Ÿè¡Œ
    - PERF_TEST_SCALE=large: å…¨è¦æ¨¡å®Ÿè¡Œ
    """
    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨±å¯ã•ã‚ŒãŸè¦æ¨¡ã‚’å–å¾—
    allowed_scale = os.getenv("PERF_TEST_SCALE", "small")

    # ã‚¹ã‚­ãƒƒãƒ—åˆ¤å®š
    if scale_name == "large" and allowed_scale != "large":
        pytest.skip(f"Large scale test skipped (PERF_TEST_SCALE={allowed_scale})")

    if scale_name == "medium" and allowed_scale == "small":
        pytest.skip(f"Medium scale test skipped (PERF_TEST_SCALE={allowed_scale})")

    # ãƒ†ã‚¹ãƒˆè¦æ¨¡ã‚’å–å¾—
    scale = get_test_scale(scale_name)

    print("\n" + "=" * 80)
    print(f"ğŸ“Š ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ: {scale.description}")
    print(f"   äº‹æ¥­æ‰€æ•°: {scale.offices}")
    print(f"   ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {scale.timeout}ç§’")
    print("=" * 80)

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™
    if scale_name == "small":
        # æ—¢å­˜ã®smallãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£ã‚’ä½¿ç”¨
        test_data = await performance_test_data_small(db_session)
    elif scale_name == "medium":
        # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆåˆ©ç”¨
        test_data = await performance_test_data_medium_snapshot(db_session)
    else:
        # largeã¯æ‰‹å‹•ç”Ÿæˆï¼ˆæ™‚é–“ãŒã‹ã‹ã‚‹ï¼‰
        pytest.skip("Large scale test requires manual setup")

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    import time
    from app.tasks.deadline_notification import send_deadline_alert_emails

    start_time = time.time()

    result = await send_deadline_alert_emails(db=db_session, dry_run=True)

    elapsed = time.time() - start_time

    # çµæœæ¤œè¨¼
    print(f"\nğŸ“ˆ ãƒ†ã‚¹ãƒˆçµæœ:")
    print(f"   å‡¦ç†æ™‚é–“: {elapsed:.1f}ç§’")
    print(f"   é€ä¿¡ãƒ¡ãƒ¼ãƒ«æ•°: {result['email_sent']}ä»¶")
    print(f"   1äº‹æ¥­æ‰€ã‚ãŸã‚Š: {elapsed / scale.offices:.2f}ç§’")

    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒã‚§ãƒƒã‚¯
    assert elapsed < scale.timeout, (
        f"Processing time exceeded timeout: {elapsed:.1f}s > {scale.timeout}s"
    )

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™ãƒã‚§ãƒƒã‚¯
    if scale_name == "medium":
        # ä¸­è¦æ¨¡: 5åˆ†ä»¥å†…
        assert elapsed < 300, f"Medium scale should complete in 5 minutes, took {elapsed:.1f}s"
```

---

### Day 10: Week 2 çµ±åˆãƒ†ã‚¹ãƒˆãƒ»æœ€çµ‚æ¤œè¨¼

#### ã‚¿ã‚¹ã‚¯6.1: å…¨ä½“çµ±åˆãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ

**ãƒ†ã‚¹ãƒˆå†…å®¹**:
```bash
# å°è¦æ¨¡ãƒ†ã‚¹ãƒˆï¼ˆæ¯å›å®Ÿè¡Œï¼‰
PERF_TEST_SCALE=small pytest tests/performance/ -v -m performance

# ä¸­è¦æ¨¡ãƒ†ã‚¹ãƒˆï¼ˆã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆåˆ©ç”¨ï¼‰
PERF_TEST_SCALE=medium pytest tests/performance/ -v -m performance

# ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆç”Ÿæˆ
docker exec keikakun_app-backend-1 python -m tests.performance.generate_snapshots
```

#### ã‚¿ã‚¹ã‚¯6.2: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šãƒ¬ãƒãƒ¼ãƒˆä½œæˆ

**æ¸¬å®šé …ç›®**:
- [ ] ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆé€Ÿåº¦ï¼ˆ100äº‹æ¥­æ‰€ç”Ÿæˆæ™‚é–“ï¼‰
- [ ] ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãƒªã‚¹ãƒˆã‚¢é€Ÿåº¦
- [ ] å°è¦æ¨¡ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚é–“ï¼ˆ10äº‹æ¥­æ‰€ï¼‰
- [ ] ä¸­è¦æ¨¡ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚é–“ï¼ˆ100äº‹æ¥­æ‰€ï¼‰
- [ ] DBã‚¯ã‚¨ãƒªæ•°ã®ç¢ºèª
- [ ] ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç¢ºèª

---

## ğŸ“Š æˆåŠŸåŸºæº–

### å¿…é ˆåŸºæº–ï¼ˆMust Haveï¼‰

| é …ç›® | ç›®æ¨™ | ç¾çŠ¶ | é”æˆ |
|------|------|------|------|
| 100äº‹æ¥­æ‰€ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ | < 5åˆ† | 60åˆ† | â³ |
| ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãƒªã‚¹ãƒˆã‚¢ | < 30ç§’ | N/A | â³ |
| 100äº‹æ¥­æ‰€ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ | < 5åˆ† | N/A | â³ |
| DBã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼ | 0ä»¶ | ç™ºç”Ÿä¸­ | â³ |

### æ¨å¥¨åŸºæº–ï¼ˆShould Haveï¼‰

| é …ç›® | ç›®æ¨™ | å‚™è€ƒ |
|------|------|------|
| 10äº‹æ¥­æ‰€ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ | < 30ç§’ | CI/CDçµ±åˆç”¨ |
| ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆç®¡ç† | è‡ªå‹•åŒ– | å¤ã„ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå‰Šé™¤ |
| ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ | å®Œå‚™ | ä½¿ç”¨æ–¹æ³•ã€ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚° |

---

## ğŸ¯ ãƒªã‚¹ã‚¯ç®¡ç†

### è­˜åˆ¥ã•ã‚ŒãŸãƒªã‚¹ã‚¯

| ãƒªã‚¹ã‚¯ | å½±éŸ¿ | ç¢ºç‡ | å¯¾ç­– |
|--------|------|------|------|
| ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆã®è¤‡é›‘æ€§ | ğŸŸ¡ ä¸­ | ğŸŸ¡ ä¸­ | æ®µéšçš„å®Ÿè£…ã€å˜ä½“ãƒ†ã‚¹ãƒˆ |
| DBè¨­å®šã®äº’æ›æ€§å•é¡Œ | ğŸŸ  é«˜ | ğŸŸ¢ ä½ | å°‚ç”¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½¿ç”¨ |
| ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã®å®¹é‡ | ğŸŸ¡ ä¸­ | ğŸŸ¡ ä¸­ | åœ§ç¸®ã€å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ |
| å®Ÿè£…é…å»¶ | ğŸŸ  é«˜ | ğŸŸ¡ ä¸­ | å„ªå…ˆåº¦ä»˜ã‘ã€ã‚¹ã‚³ãƒ¼ãƒ—èª¿æ•´ |

### å¯¾ç­–è©³ç´°

1. **å®Ÿè£…é…å»¶ãƒªã‚¹ã‚¯**
   - Week 1 Day 3ã§ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³ç¢ºèª
   - é…å»¶æ™‚ã¯ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆæ©Ÿèƒ½ã‚’å¾Œå›ã—
   - æœ€ä½é™ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆã¯å®Œäº†ã•ã›ã‚‹

2. **DBè¨­å®šå•é¡Œ**
   - å°‚ç”¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§å½±éŸ¿ç¯„å›²ã‚’é™å®š
   - æ—¢å­˜ãƒ†ã‚¹ãƒˆã«å½±éŸ¿ã‚’ä¸ãˆãªã„
   - ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯èƒ½ãªè¨­å®š

---

## ğŸ“š ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ

### ä½œæˆã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

1. **å®Ÿè£…ã‚¬ã‚¤ãƒ‰**ï¼ˆæœ¬ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼‰
   - âœ… å®Œäº†

2. **ä½¿ç”¨æ–¹æ³•ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**
   - ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆç”Ÿæˆæ–¹æ³•
   - ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ–¹æ³•
   - ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

3. **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆ**
   - æ¸¬å®šçµæœ
   - æ”¹å–„åŠ¹æœ
   - æ¨å¥¨äº‹é …

---

## ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

### å®Ÿè£…å¾Œã®å±•é–‹

1. **CI/CDçµ±åˆ**ï¼ˆå°†æ¥ï¼‰
   - GitHub Actionså¯¾å¿œ
   - è‡ªå‹•ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
   - æ€§èƒ½åŠ£åŒ–æ¤œçŸ¥

2. **ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**ï¼ˆå°†æ¥ï¼‰
   - Grafanaçµ±åˆ
   - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–
   - ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š

3. **ã•ã‚‰ãªã‚‹æœ€é©åŒ–**ï¼ˆå°†æ¥ï¼‰
   - ä¸¦åˆ—ç”Ÿæˆ
   - å¢—åˆ†ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
   - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

---

## âœ… å®Ÿè£…é–‹å§‹ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### äº‹å‰æº–å‚™
- [ ] Dockerç’°å¢ƒã®ç¢ºèª
- [ ] PostgreSQL 15ã®ç¢ºèª
- [ ] ãƒ†ã‚¹ãƒˆDBæ¥ç¶šã®ç¢ºèª
- [ ] ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ã®ç¢ºèªï¼ˆã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆç”¨: æœ€ä½10GBï¼‰

### Week 1 æº–å‚™
- [ ] `tests/performance/bulk_factories.py` ä½œæˆ
- [ ] `tests/performance/snapshot_manager.py` ä½œæˆ
- [ ] `tests/performance/generate_snapshots.py` ä½œæˆ

### Week 2 æº–å‚™
- [ ] `docker-compose.test.yml` ä½œæˆ
- [ ] `tests/performance/config.py` ä½œæˆ
- [ ] `tests/performance/test_scales.py` ä½œæˆ

---

**è¨ˆç”»ç­–å®šå®Œäº†æ—¥**: 2026-02-09
**å®Ÿè£…é–‹å§‹äºˆå®š**: å³åº§
**å®Œäº†äºˆå®š**: 2é€±é–“å¾Œï¼ˆ2026-02-23ï¼‰
**æœŸå¾…åŠ¹æœ**: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ12å€é«˜é€ŸåŒ–ã€100äº‹æ¥­æ‰€ãƒ†ã‚¹ãƒˆå®Ÿè¡Œå¯èƒ½åŒ–
