# ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä¸è¶³åˆ†æ - ãƒ†ã‚¹ãƒˆã‚¤ãƒ³ãƒ•ãƒ©å®Ÿè£…

**å¯¾è±¡**: Day 1-2 å®Œäº†å®Ÿè£…ï¼ˆãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ + ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆç®¡ç†ï¼‰
**ä½œæˆæ—¥**: 2026-02-11
**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: ğŸ“‹ Analysis Complete - Implementation Pending

---

## ğŸ“‹ Executive Summary

Day 1-2ã§å®Ÿè£…ã—ãŸãƒ†ã‚¹ãƒˆã‚¤ãƒ³ãƒ•ãƒ©ï¼ˆ`bulk_factories.py` + `snapshot_manager.py`ï¼‰ã¯**ãƒãƒƒãƒ”ãƒ¼ãƒ‘ã‚¹**ã¯å®Œå…¨ã«å‹•ä½œã™ã‚‹ãŒã€**ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãŒä¸è¶³**ã—ã¦ã„ã‚‹ã€‚

### é‡å¤§åº¦åˆ¥ã®ä¸è¶³åˆ†é¡

| é‡å¤§åº¦ | ä»¶æ•° | å½±éŸ¿ç¯„å›² |
|--------|------|----------|
| **Critical** ğŸ”´ | 8 | ãƒ‡ãƒ¼ã‚¿ç ´æã€éƒ¨åˆ†çš„ãªçŠ¶æ…‹ã€å¾©æ—§ä¸å¯ |
| **High** ğŸŸ  | 12 | æ“ä½œå¤±æ•—ã€ä¸æ˜ç¢ºãªã‚¨ãƒ©ãƒ¼ã€ãƒ‡ãƒãƒƒã‚°å›°é›£ |
| **Medium** ğŸŸ¡ | 6 | åˆ©ä¾¿æ€§ä½ä¸‹ã€æ‰‹å‹•ä»‹å…¥å¿…è¦ |
| **Total** | **26** | - |

---

## ğŸ”´ Critical Priority (Priority 1)

### 1. Snapshot Creation - Partial Write Failure

**å•é¡Œ**: JSONæ›¸ãè¾¼ã¿ãŒé€”ä¸­ã§å¤±æ•—ã™ã‚‹ã¨ã€**ç ´æã—ãŸã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«**ãŒæ®‹ã‚‹

**ç¾çŠ¶ã‚³ãƒ¼ãƒ‰** (`snapshot_manager.py:133-134`):
```python
with open(snapshot_path, "w", encoding="utf-8") as f:
    json.dump(snapshot_data, f, ensure_ascii=False, indent=2, default=str)
```

**ã‚·ãƒŠãƒªã‚ª**:
1. JSONæ›¸ãè¾¼ã¿é–‹å§‹
2. **ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ä¸è¶³**ã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ
3. éƒ¨åˆ†çš„ã«æ›¸ãè¾¼ã¾ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒæ®‹ã‚‹
4. æ¬¡å›ã®å¾©å…ƒæ™‚ã«**ç ´æã—ãŸJSONã‚’èª­ã¿è¾¼ã‚“ã§å¤±æ•—**

**å½±éŸ¿**:
- âœ… `snapshot_exists("name")` â†’ `True`ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã¯å­˜åœ¨ï¼‰
- âŒ `restore_snapshot("name")` â†’ **JSONDecodeError**ï¼ˆç ´æï¼‰
- ğŸš¨ **ãƒ†ã‚¹ãƒˆãŒå®Œå…¨ã«åœæ­¢**ï¼ˆå¾©æ—§ä¸å¯ï¼‰

**æ¨å¥¨å¯¾å¿œ**:
```python
import tempfile
import shutil

# ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿ â†’ æˆåŠŸã—ãŸã‚‰atomic move
temp_path = snapshot_path.with_suffix(".tmp")
try:
    with open(temp_path, "w", encoding="utf-8") as f:
        json.dump(snapshot_data, f, ensure_ascii=False, indent=2, default=str)

    # Atomic moveï¼ˆæˆåŠŸæ™‚ã®ã¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚Œã‚‹ï¼‰
    shutil.move(temp_path, snapshot_path)
except Exception as e:
    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    if temp_path.exists():
        temp_path.unlink()
    logger.error(f"âŒ Snapshot creation failed: {e}")
    raise
```

---

### 2. Snapshot Restoration - Partial Data Restoration

**å•é¡Œ**: ãƒ†ãƒ¼ãƒ–ãƒ«å¾©å…ƒãŒé€”ä¸­ã§å¤±æ•—ã™ã‚‹ã¨ã€**éƒ¨åˆ†çš„ã«ãƒ‡ãƒ¼ã‚¿ãŒå¾©å…ƒã•ã‚ŒãŸçŠ¶æ…‹**ãŒæ®‹ã‚‹

**ç¾çŠ¶ã‚³ãƒ¼ãƒ‰** (`snapshot_manager.py:200-228`):
```python
for table in tables_order:
    rows = snapshot_data["tables"].get(table, [])
    for row in rows:
        # ...
        await db.execute(query, processed_row)  # âŒ ã‚¨ãƒ©ãƒ¼ã§ã“ã“ã§åœæ­¢
    logger.info(f"  Restored {len(rows)} rows to {table}")

await db.commit()  # â† ã“ã“ã«åˆ°é”ã—ãªã„å ´åˆã€éƒ¨åˆ†çš„ãªçŠ¶æ…‹ãŒæ®‹ã‚‹
```

**ã‚·ãƒŠãƒªã‚ª**:
```
staffs          â†’ âœ… æŒ¿å…¥æˆåŠŸ (1,000ä»¶)
offices         â†’ âœ… æŒ¿å…¥æˆåŠŸ (100ä»¶)
office_staffs   â†’ âŒ å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„é•åã§ã‚¨ãƒ©ãƒ¼
welfare_recipients â†’ âš ï¸ å®Ÿè¡Œã•ã‚Œãªã„
...
```

**å½±éŸ¿**:
- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«**ä¸å®Œå…¨ãªçŠ¶æ…‹**ãŒæ®‹ã‚‹
- æ¬¡å›ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚ã«**äºˆæœŸã—ãªã„ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨**
- `clean_existing=True`ã§ã‚‚å‰Šé™¤ã•ã‚Œãªã„ï¼ˆä¸€éƒ¨ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿æ®‹å­˜ï¼‰

**æ¨å¥¨å¯¾å¿œ**:
```python
try:
    for table in tables_order:
        rows = snapshot_data["tables"].get(table, [])
        for row in rows:
            # ... æŒ¿å…¥å‡¦ç† ...
            await db.execute(query, processed_row)
        logger.info(f"  Restored {len(rows)} rows to {table}")

    await db.commit()  # âœ… All or nothing
    logger.info(f"âœ… Snapshot restored: {name}")

except Exception as e:
    logger.error(f"âŒ Snapshot restoration failed: {e}")
    await db.rollback()  # å…¨ã¦ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
    raise RuntimeError(f"Snapshot restoration failed at table '{table}': {e}")
```

---

### 3. Bulk Insert - No Rollback on Failure

**å•é¡Œ**: ãƒãƒƒãƒæŒ¿å…¥ãŒé€”ä¸­ã§å¤±æ•—ã™ã‚‹ã¨ã€**éƒ¨åˆ†çš„ã«ãƒ‡ãƒ¼ã‚¿ãŒæŒ¿å…¥ã•ã‚ŒãŸçŠ¶æ…‹**ãŒæ®‹ã‚‹

**ç¾çŠ¶ã‚³ãƒ¼ãƒ‰** (`bulk_factories.py:85-91`):
```python
for i in range(0, len(offices), batch_size):
    batch = offices[i:i + batch_size]
    db.add_all(batch)
    await db.flush()  # âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã€å‰ã®ãƒãƒƒãƒã¯æ—¢ã«flushæ¸ˆã¿

await db.commit()  # â† ã“ã“ã«åˆ°é”ã—ãªã„
```

**ã‚·ãƒŠãƒªã‚ª**:
```
Batch 1 (offices 0-499)    â†’ âœ… flushæˆåŠŸ
Batch 2 (offices 500-999)  â†’ âŒ uniqueåˆ¶ç´„é•åã§ã‚¨ãƒ©ãƒ¼
Batch 3 (offices 1000-...) â†’ âš ï¸ å®Ÿè¡Œã•ã‚Œãªã„
```

**å½±éŸ¿**:
- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«**500ä»¶ã®äº‹æ¥­æ‰€**ãŒæ®‹å­˜ï¼ˆéƒ¨åˆ†çš„ï¼‰
- æ¬¡å›å®Ÿè¡Œæ™‚ã«uniqueåˆ¶ç´„é•åãŒç™ºç”Ÿ
- ãƒ†ã‚¹ãƒˆã®ç‹¬ç«‹æ€§ãŒç ´å£Šã•ã‚Œã‚‹

**æ¨å¥¨å¯¾å¿œ**:
```python
try:
    for i in range(0, len(offices), batch_size):
        batch = offices[i:i + batch_size]
        db.add_all(batch)
        await db.flush()

    await db.commit()  # âœ… All or nothing
    return offices

except Exception as e:
    logger.error(f"âŒ Bulk insert failed at batch {i // batch_size}: {e}")
    await db.rollback()  # å…¨ã¦ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
    raise RuntimeError(f"Failed to create offices: {e}")
```

---

### 4. Foreign Key Constraint Violation - Unclear Error

**å•é¡Œ**: å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„é•åæ™‚ã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒ**DBå†…éƒ¨ã‚¨ãƒ©ãƒ¼**ã®ã¾ã¾ï¼ˆãƒ‡ãƒãƒƒã‚°å›°é›£ï¼‰

**ç¾çŠ¶**: ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ã¨ã€PostgreSQLã®ç”Ÿã‚¨ãƒ©ãƒ¼ãŒãã®ã¾ã¾å‡ºåŠ›ã•ã‚Œã‚‹
```
sqlalchemy.exc.IntegrityError: (asyncpg.exceptions.ForeignKeyViolationError)
insert or update on table "offices" violates foreign key constraint "offices_created_by_fkey"
DETAIL: Key (created_by)=(uuid) is not present in table "staffs".
```

**ã‚·ãƒŠãƒªã‚ª**: `bulk_create_offices()`ã§`created_by`ã®ã‚·ã‚¹ãƒ†ãƒ ã‚¹ã‚¿ãƒƒãƒ•ãŒå­˜åœ¨ã—ãªã„

**å½±éŸ¿**:
- ãƒ†ã‚¹ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãŒ**åŸå› ã‚’ç†è§£ã§ããªã„**
- ãƒ‡ãƒãƒƒã‚°ã«æ™‚é–“ãŒã‹ã‹ã‚‹
- ã‚¨ãƒ©ãƒ¼åŸå› ãŒã‚³ãƒ¼ãƒ‰ã‹ã‚‰è¿½è·¡å›°é›£

**æ¨å¥¨å¯¾å¿œ**:
```python
from sqlalchemy.exc import IntegrityError

try:
    await db.commit()
except IntegrityError as e:
    await db.rollback()

    # åˆ†ã‹ã‚Šã‚„ã™ã„ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    if "foreign key constraint" in str(e).lower():
        raise RuntimeError(
            f"å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„é•å: å‚ç…§å…ˆã®ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚"
            f"ã‚·ã‚¹ãƒ†ãƒ ã‚¹ã‚¿ãƒƒãƒ•ãŒä½œæˆã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n"
            f"è©³ç´°: {e}"
        )
    elif "unique constraint" in str(e).lower():
        raise RuntimeError(
            f"ä¸€æ„åˆ¶ç´„é•å: é‡è¤‡ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒæ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚\n"
            f"è©³ç´°: {e}"
        )
    else:
        raise
```

---

### 5. Snapshot Restoration - Corrupted JSON

**å•é¡Œ**: JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒç ´æã—ã¦ã„ã‚‹å ´åˆã€**æ˜ç¢ºãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãªã—**

**ç¾çŠ¶ã‚³ãƒ¼ãƒ‰** (`snapshot_manager.py:179-180`):
```python
with open(snapshot_path, "r", encoding="utf-8") as f:
    snapshot_data = json.load(f)  # âŒ JSONDecodeErrorãŒç”Ÿã§å‡ºã‚‹
```

**ã‚·ãƒŠãƒªã‚ª**:
1. ãƒ‡ã‚£ã‚¹ã‚¯éšœå®³ã§JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒç ´æ
2. `restore_snapshot()`å®Ÿè¡Œ
3. **JSONDecodeError**: `Expecting value: line 1 column 1 (char 0)`

**å½±éŸ¿**:
- ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰**åŸå› ãŒä¸æ˜**
- æ‰‹å‹•ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
- ãƒ†ã‚¹ãƒˆãŒåœæ­¢ã™ã‚‹

**æ¨å¥¨å¯¾å¿œ**:
```python
try:
    with open(snapshot_path, "r", encoding="utf-8") as f:
        snapshot_data = json.load(f)
except json.JSONDecodeError as e:
    raise RuntimeError(
        f"ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ '{name}' ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒç ´æã—ã¦ã„ã¾ã™ã€‚\n"
        f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: {snapshot_path}\n"
        f"å‰Šé™¤ã—ã¦ã‹ã‚‰å†ä½œæˆã—ã¦ãã ã•ã„ã€‚\n"
        f"è©³ç´°: {e}"
    )
except Exception as e:
    raise RuntimeError(f"ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
```

---

### 6. Schema Mismatch - No Validation

**å•é¡Œ**: ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆä½œæˆæ™‚ã¨DBå†ã‚¹ã‚­ãƒ¼ãƒãŒç•°ãªã‚‹å ´åˆã€**å¾©å…ƒæ™‚ã«ã‚¨ãƒ©ãƒ¼**

**ã‚·ãƒŠãƒªã‚ª**:
1. 2026-01-01: ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆä½œæˆï¼ˆ`offices`ã«`region`åˆ—ãªã—ï¼‰
2. 2026-01-15: DBãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œï¼ˆ`offices.region NOT NULL`ã‚’è¿½åŠ ï¼‰
3. 2026-01-16: ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå¾©å…ƒ â†’ **NOT NULLåˆ¶ç´„é•å**

**ç¾çŠ¶**: ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ãŒã€åŸå› ãŒä¸æ˜ç¢º

**å½±éŸ¿**:
- å¤ã„ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãŒ**ä½¿ç”¨ä¸å¯èƒ½**ã«ãªã‚‹
- ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰åŸå› ãŒåˆ†ã‹ã‚‰ãªã„

**æ¨å¥¨å¯¾å¿œ**:
```python
# ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã«ã‚¹ã‚­ãƒ¼ãƒãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä¿å­˜
snapshot_data = {
    "schema_version": "1.0.0",  # Alembicãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³
    "name": name,
    "created_at": datetime.now().isoformat(),
    ...
}

# å¾©å…ƒæ™‚ã«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯
current_version = await get_current_schema_version(db)
snapshot_version = snapshot_data.get("schema_version", "unknown")

if snapshot_version != current_version:
    logger.warning(
        f"âš ï¸ ã‚¹ã‚­ãƒ¼ãƒãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒä¸€è‡´ã—ã¾ã›ã‚“: "
        f"ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ={snapshot_version}, ç¾åœ¨={current_version}"
    )
    # Continue with caution or raise error
```

---

### 7. DB Connection Loss - No Retry

**å•é¡Œ**: DBæ¥ç¶šãŒåˆ‡ã‚ŒãŸå ´åˆã€**å³åº§ã«å¤±æ•—**ï¼ˆä¸€æ™‚çš„ãªã‚¨ãƒ©ãƒ¼ã«å¯¾å¿œã§ããªã„ï¼‰

**ç¾çŠ¶**: DBã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹ã¨å³åº§ã«ã‚¨ãƒ©ãƒ¼çµ‚äº†

**ã‚·ãƒŠãƒªã‚ª**:
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¸€æ™‚æ–­
- DBã‚µãƒ¼ãƒãƒ¼ã®å†èµ·å‹•
- æ¥ç¶šãƒ—ãƒ¼ãƒ«ã®æ¯æ¸‡

**å½±éŸ¿**:
- **9åˆ†é–“ã®ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ**ãŒæœ€å¾Œã®1ç§’ã§å¤±æ•—
- å…¨ã¦ã‚„ã‚Šç›´ã—

**æ¨å¥¨å¯¾å¿œ**:
```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),           # æœ€å¤§3å›
    wait=wait_exponential(multiplier=2),  # 2ç§’ â†’ 4ç§’ â†’ 8ç§’
    reraise=True
)
async def bulk_create_offices_with_retry(db: AsyncSession, count: int):
    return await bulk_create_offices(db, count)
```

---

### 8. Disk Space Check - No Validation

**å•é¡Œ**: ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆä½œæˆå‰ã«**ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ã‚’ãƒã‚§ãƒƒã‚¯ã—ãªã„**

**ã‚·ãƒŠãƒªã‚ª**:
1. 100äº‹æ¥­æ‰€ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆä½œæˆé–‹å§‹ï¼ˆäºˆæƒ³ã‚µã‚¤ã‚º: 500MBï¼‰
2. ãƒ‡ã‚£ã‚¹ã‚¯æ®‹å®¹é‡: 100MB
3. **é€”ä¸­ã§å®¹é‡ä¸è¶³**ã‚¨ãƒ©ãƒ¼ â†’ ç ´æã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒæ®‹ã‚‹

**å½±éŸ¿**:
- ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã«9åˆ†ã‹ã‘ãŸå¾Œã§å¤±æ•—
- ç ´æãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¿…è¦

**æ¨å¥¨å¯¾å¿œ**:
```python
import shutil

def check_disk_space(path: Path, required_mb: int = 1000):
    """ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€ä½1GBå¿…è¦ï¼‰"""
    stat = shutil.disk_usage(path)
    free_mb = stat.free / (1024 * 1024)

    if free_mb < required_mb:
        raise RuntimeError(
            f"ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ä¸è¶³: ç©ºãå®¹é‡ {free_mb:.0f}MB < å¿…è¦å®¹é‡ {required_mb}MB"
        )

# ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆä½œæˆå‰ã«ãƒã‚§ãƒƒã‚¯
check_disk_space(SNAPSHOT_DIR, required_mb=1000)
```

---

## ğŸŸ  High Priority (Priority 2)

### 9. Bulk Insert - Invalid Parameters

**å•é¡Œ**: ä¸æ­£ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆè² ã®æ•°ã€0ï¼‰ã‚’æ¸¡ã—ã¦ã‚‚**ãƒã‚§ãƒƒã‚¯ãªã—**

**ç¾çŠ¶ã‚³ãƒ¼ãƒ‰**:
```python
async def bulk_create_offices(db: AsyncSession, count: int, batch_size: int = 500):
    # count=-100 ã§ã‚‚å®Ÿè¡Œã•ã‚Œã‚‹ï¼
```

**å½±éŸ¿**:
- `count=-100` â†’ ç©ºã®ãƒªã‚¹ãƒˆãŒè¿”ã‚‹ï¼ˆã‚¨ãƒ©ãƒ¼ãªã—ï¼‰
- `batch_size=0` â†’ **ZeroDivisionError**
- ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ãŸåŸå› ãŒåˆ†ã‹ã‚‰ãªã„

**æ¨å¥¨å¯¾å¿œ**:
```python
if count <= 0:
    raise ValueError(f"count must be positive: {count}")
if batch_size <= 0:
    raise ValueError(f"batch_size must be positive: {batch_size}")
if batch_size > 1000:
    logger.warning(f"Large batch_size may cause memory issues: {batch_size}")
```

---

### 10. Progress Tracking - Long Operations

**å•é¡Œ**: 9åˆ†é–“ã®ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­ã€**é€²æ—ãŒä¸æ˜**ï¼ˆå‡¦ç†ãŒæ­¢ã¾ã£ãŸã®ã‹åˆ¤æ–­ã§ããªã„ï¼‰

**ç¾çŠ¶**: æœ€å¾Œã«ã®ã¿ãƒ­ã‚°å‡ºåŠ›
```python
# 9åˆ†é–“æ²ˆé»™...
logger.info(f"âœ… Snapshot created: {snapshot_path}")
```

**å½±éŸ¿**:
- ãƒ†ã‚¹ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãŒ**å‡¦ç†ä¸­ã‹åœæ­¢ä¸­ã‹åˆ¤æ–­ã§ããªã„**
- CI/CDã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã™ã‚‹å¯èƒ½æ€§

**æ¨å¥¨å¯¾å¿œ**:
```python
from tqdm import tqdm

# ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
for i in tqdm(range(0, len(offices), batch_size), desc="Creating offices"):
    batch = offices[i:i + batch_size]
    db.add_all(batch)
    await db.flush()

# ã¾ãŸã¯ãƒ­ã‚°ãƒ™ãƒ¼ã‚¹
total_batches = (count + batch_size - 1) // batch_size
for batch_idx, i in enumerate(range(0, len(offices), batch_size)):
    batch = offices[i:i + batch_size]
    db.add_all(batch)
    await db.flush()
    logger.info(f"Progress: {batch_idx + 1}/{total_batches} batches completed")
```

---

### 11. Snapshot List - JSON Parse Error

**å•é¡Œ**: `list_snapshots()`ã§ç ´æã—ãŸJSONãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã¨**å…¨ä½“ãŒå¤±æ•—**

**ç¾çŠ¶ã‚³ãƒ¼ãƒ‰** (`snapshot_manager.py:276-285`):
```python
for snapshot_file in SNAPSHOT_DIR.glob("*.json"):
    with open(snapshot_file, "r", encoding="utf-8") as f:
        data = json.load(f)  # âŒ 1ã¤ã§ã‚‚ç ´æã—ã¦ã„ã‚‹ã¨å…¨ä½“ãŒå¤±æ•—
```

**å½±éŸ¿**:
- 1ã¤ã®ç ´æãƒ•ã‚¡ã‚¤ãƒ«ã§**å…¨ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãŒè¡¨ç¤ºã§ããªã„**
- ãƒ†ã‚¹ãƒˆãŒå®Œå…¨ã«åœæ­¢

**æ¨å¥¨å¯¾å¿œ**:
```python
snapshots = []
errors = []

for snapshot_file in SNAPSHOT_DIR.glob("*.json"):
    try:
        with open(snapshot_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        snapshots.append(SnapshotMetadata.from_dict({...}))
    except Exception as e:
        errors.append(f"{snapshot_file.name}: {e}")
        logger.warning(f"âš ï¸ Skipping corrupted snapshot: {snapshot_file.name}")

if errors:
    logger.warning(f"âš ï¸ {len(errors)} snapshots could not be loaded:\n" + "\n".join(errors))

return snapshots
```

---

### 12. Snapshot Delete - File Lock

**å•é¡Œ**: ãƒ•ã‚¡ã‚¤ãƒ«ãŒãƒ­ãƒƒã‚¯ã•ã‚Œã¦ã„ã‚‹å ´åˆã€**å‰Šé™¤å¤±æ•—**ã™ã‚‹ãŒæ˜ç¢ºãªã‚¨ãƒ©ãƒ¼ãªã—

**ç¾çŠ¶ã‚³ãƒ¼ãƒ‰** (`snapshot_manager.py:308`):
```python
snapshot_path.unlink()  # âŒ PermissionErrorãŒç”Ÿã§å‡ºã‚‹
```

**æ¨å¥¨å¯¾å¿œ**:
```python
try:
    snapshot_path.unlink()
    logger.info(f"ğŸ—‘ï¸ Snapshot deleted: {name}")
    return True
except PermissionError:
    raise RuntimeError(
        f"ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ '{name}' ã¯ä½¿ç”¨ä¸­ã®ãŸã‚å‰Šé™¤ã§ãã¾ã›ã‚“ã€‚\n"
        f"ä»–ã®ãƒ—ãƒ­ã‚»ã‚¹ã§ä½¿ç”¨ã—ã¦ã„ãªã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
    )
except Exception as e:
    raise RuntimeError(f"ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
```

---

### 13. Concurrent Snapshot Creation

**å•é¡Œ**: è¤‡æ•°ãƒ—ãƒ­ã‚»ã‚¹ã§åŒæ™‚ã«åŒã˜ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’ä½œæˆã™ã‚‹ã¨**ç«¶åˆ**

**ã‚·ãƒŠãƒªã‚ª**:
1. ãƒ—ãƒ­ã‚»ã‚¹A: `create_snapshot("test")` é–‹å§‹
2. ãƒ—ãƒ­ã‚»ã‚¹B: `create_snapshot("test")` é–‹å§‹
3. ãƒ—ãƒ­ã‚»ã‚¹A: ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯ â†’ Not found â†’ æ›¸ãè¾¼ã¿é–‹å§‹
4. ãƒ—ãƒ­ã‚»ã‚¹B: ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯ â†’ Not found â†’ æ›¸ãè¾¼ã¿é–‹å§‹
5. **ä¸¡æ–¹ãŒåŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿** â†’ ç ´æ

**æ¨å¥¨å¯¾å¿œ**:
```python
import fcntl

# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯
lock_path = SNAPSHOT_DIR / f"{name}.lock"
try:
    with open(lock_path, "w") as lock_file:
        fcntl.flock(lock_file.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)

        # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆä½œæˆ
        # ...

finally:
    if lock_path.exists():
        lock_path.unlink()
```

---

### 14-20. Additional High Priority Issues

**14. JSONB Serialization Error**:
- `notification_preferences`ã®ä¸€éƒ¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒä¸æ­£ãªå‹ â†’ JSONåŒ–å¤±æ•—
- æ¨å¥¨: `default=str`ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ + ãƒ­ã‚°è­¦å‘Š

**15. Memory Overflow - Large Datasets**:
- 1,000äº‹æ¥­æ‰€ Ã— 100ã‚¹ã‚¿ãƒƒãƒ• = 100,000ä»¶ã‚’ä¸€åº¦ã«ãƒ¡ãƒ¢ãƒªå±•é–‹ â†’ OutOfMemory
- æ¨å¥¨: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯ + ãƒãƒ£ãƒ³ã‚¯å‡¦ç†

**16. Flush Timeout**:
- å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®flushãŒ**é•·æ™‚é–“ã‹ã‹ã‚‹**å ´åˆã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæœªè¨­å®š
- æ¨å¥¨: `asyncio.wait_for(db.flush(), timeout=60)`

**17. Snapshot Restoration - Missing Tables**:
- ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã«å«ã¾ã‚Œã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ãŒDBã«å­˜åœ¨ã—ãªã„ â†’ å¾©å…ƒå¤±æ•—
- æ¨å¥¨: ãƒ†ãƒ¼ãƒ–ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯

**18. Audit Log Failure**:
- ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆä½œæˆ/å¾©å…ƒã®ç›£æŸ»ãƒ­ã‚°ãŒå¤±æ•—ã—ã¦ã‚‚**å‡¦ç†ãŒç¶™ç¶š**
- æ¨å¥¨: ç›£æŸ»ãƒ­ã‚°å¤±æ•—æ™‚ã®æ˜ç¤ºçš„ãªã‚¨ãƒ©ãƒ¼ã¾ãŸã¯è­¦å‘Š

**19. Unique Constraint Violation - Retry**:
- å¶ç™ºçš„ãªãƒ¦ãƒ‹ãƒ¼ã‚¯åˆ¶ç´„é•åï¼ˆUUIDã®è¡çªãªã©ï¼‰ã«å¯¾ã™ã‚‹ãƒªãƒˆãƒ©ã‚¤ãªã—
- æ¨å¥¨: tenacityã§ãƒªãƒˆãƒ©ã‚¤ï¼ˆæœ€å¤§3å›ï¼‰

**20. Session Expiration**:
- é•·æ™‚é–“ã®å‡¦ç†ä¸­ã«DBã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒæœŸé™åˆ‡ã‚Œ â†’ æ¥ç¶šã‚¨ãƒ©ãƒ¼
- æ¨å¥¨: ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®keepaliveè¨­å®š

---

## ğŸŸ¡ Medium Priority (Priority 3)

### 21. Snapshot Overwrite Protection

**å•é¡Œ**: èª¤ã£ã¦æ—¢å­˜ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’ä¸Šæ›¸ãã™ã‚‹å±é™ºæ€§

**ç¾çŠ¶**: `ValueError`ã‚’å‡ºã™ãŒã€**force=True**ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒãªã„

**æ¨å¥¨å¯¾å¿œ**:
```python
async def create_snapshot(
    db: AsyncSession,
    name: str,
    description: str = "",
    overwrite: bool = False  # æ˜ç¤ºçš„ãªä¸Šæ›¸ãè¨±å¯
):
    if snapshot_path.exists() and not overwrite:
        raise ValueError(
            f"Snapshot '{name}' already exists. "
            f"Use overwrite=True to replace it."
        )
```

---

### 22-26. Additional Medium Priority Issues

**22. Snapshot Versioning**:
- ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ãªã—ï¼ˆå¤ã„ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã®è­˜åˆ¥å›°é›£ï¼‰
- æ¨å¥¨: `name_v1.json`, `name_v2.json`ã®ã‚ˆã†ãªãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°

**23. Cleanup Strategy**:
- å¤ã„ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã®è‡ªå‹•å‰Šé™¤æ©Ÿèƒ½ãªã—ï¼ˆãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡æµªè²»ï¼‰
- æ¨å¥¨: å¤ã„ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆï¼ˆ30æ—¥ä»¥ä¸Šï¼‰ã®è‡ªå‹•å‰Šé™¤

**24. Snapshot Comparison**:
- 2ã¤ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã®å·®åˆ†æ¯”è¼ƒæ©Ÿèƒ½ãªã—
- æ¨å¥¨: `compare_snapshots(name1, name2)` æ©Ÿèƒ½

**25. Incremental Snapshot**:
- å·®åˆ†ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆæ©Ÿèƒ½ãªã—ï¼ˆæ¯å›ãƒ•ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼‰
- æ¨å¥¨: å¢—åˆ†ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå¯¾å¿œ

**26. Snapshot Compression**:
- JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒéåœ§ç¸®ï¼ˆ500MB â†’ 50MBã«åœ§ç¸®å¯èƒ½ï¼‰
- æ¨å¥¨: gzipåœ§ç¸®å¯¾å¿œ

---

## ğŸ“Š Summary Table

| ã‚«ãƒ†ã‚´ãƒª | Critical | High | Medium | Total |
|---------|----------|------|--------|-------|
| Snapshot Creation | 3 | 4 | 3 | 10 |
| Snapshot Restoration | 3 | 2 | 0 | 5 |
| Bulk Insert | 2 | 4 | 0 | 6 |
| General | 0 | 2 | 3 | 5 |
| **Total** | **8** | **12** | **6** | **26** |

---

## ğŸ¯ Recommended Implementation Order

### Phase 1: Critical Fixes (1-2 days)
1. âœ… Atomic snapshot write (Issue #1)
2. âœ… Rollback on restoration failure (Issue #2)
3. âœ… Rollback on bulk insert failure (Issue #3)
4. âœ… Friendly error messages (Issue #4)
5. âœ… Corrupted JSON handling (Issue #5)
6. âœ… Schema version check (Issue #6)
7. âœ… DB retry mechanism (Issue #7)
8. âœ… Disk space check (Issue #8)

### Phase 2: High Priority (2-3 days)
9. Parameter validation (Issue #9)
10. Progress tracking (Issue #10)
11. Robust snapshot listing (Issue #11)
12. File lock handling (Issue #12)
13. Concurrent creation protection (Issue #13)
14-20. Other high priority issues

### Phase 3: Medium Priority (Optional)
21-26. Nice-to-have features

---

## ğŸ” Testing Strategy

### Critical Error Scenarios (Must Test)

**Test 1: Disk Full During Snapshot Creation**
```python
async def test_snapshot_creation_disk_full(monkeypatch):
    """ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ä¸è¶³æ™‚ã®å‹•ä½œç¢ºèª"""
    def mock_disk_usage(path):
        return type('obj', (object,), {'free': 100})()  # 100 bytes only

    monkeypatch.setattr(shutil, 'disk_usage', mock_disk_usage)

    with pytest.raises(RuntimeError, match="ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ä¸è¶³"):
        await create_snapshot(db, "test")

    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚Œã¦ã„ãªã„ã“ã¨ã‚’ç¢ºèª
    assert not (SNAPSHOT_DIR / "test.json").exists()
```

**Test 2: DB Connection Loss During Bulk Insert**
```python
async def test_bulk_insert_connection_loss(db_session, monkeypatch):
    """DBæ¥ç¶šæ–­æ™‚ã®ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ç¢ºèª"""
    call_count = 0

    async def mock_flush_with_failure():
        nonlocal call_count
        call_count += 1
        if call_count == 2:
            raise asyncpg.exceptions.ConnectionDoesNotExistError()
        await original_flush()

    monkeypatch.setattr(db_session, 'flush', mock_flush_with_failure)

    with pytest.raises(RuntimeError):
        await bulk_create_offices(db_session, count=1000, batch_size=500)

    # éƒ¨åˆ†çš„ãªãƒ‡ãƒ¼ã‚¿ãŒæ®‹ã£ã¦ã„ãªã„ã“ã¨ã‚’ç¢ºèª
    result = await db_session.execute(
        select(func.count()).select_from(Office).where(Office.is_test_data == True)
    )
    assert result.scalar() == 0  # âœ… RollbackæˆåŠŸ
```

**Test 3: Corrupted JSON Restoration**
```python
async def test_restore_corrupted_json(db_session):
    """ç ´æJSONã‹ã‚‰ã®å¾©å…ƒã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°"""
    snapshot_path = SNAPSHOT_DIR / "corrupted.json"
    snapshot_path.write_text("{ invalid json }")

    with pytest.raises(RuntimeError, match="JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒç ´æ"):
        await restore_snapshot(db_session, "corrupted")
```

---

## ğŸ“š Related Documents

- [Day 1-2 Completion Report](./day1_2_completion_report.md)
- [Test Infrastructure Plan](./test_infrastructure_implementation_plan.md)
- [Performance Review](./review/comprehensive_review.md)

---

## ğŸ† Expected Outcome

### Before (Current State)
- âœ… ãƒãƒƒãƒ”ãƒ¼ãƒ‘ã‚¹ã¯å®Œå…¨å‹•ä½œ
- âŒ ã‚¨ãƒ©ãƒ¼æ™‚ã®å‹•ä½œãŒä¸æ˜ç¢º
- âŒ éƒ¨åˆ†çš„ãªçŠ¶æ…‹ãŒæ®‹ã‚‹å¯èƒ½æ€§
- âŒ ãƒ‡ãƒãƒƒã‚°ãŒå›°é›£

### After (With Error Handling)
- âœ… ãƒãƒƒãƒ”ãƒ¼ãƒ‘ã‚¹ã¯å®Œå…¨å‹•ä½œ
- âœ… ã‚¨ãƒ©ãƒ¼æ™‚ã«æ˜ç¢ºãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
- âœ… All-or-nothingï¼ˆåŸå­æ€§ä¿è¨¼ï¼‰
- âœ… å¾©æ—§å¯èƒ½ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- âœ… é€²æ—è¿½è·¡ã¨ãƒ‡ãƒãƒƒã‚°å®¹æ˜“æ€§

---

**ä½œæˆæ—¥**: 2026-02-11
**ä½œæˆè€…**: Claude Sonnet 4.5
**ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: ğŸ“‹ Analysis Complete - Ready for Implementation

---

**Note**: ã“ã®åˆ†æã¯ã€å®Ÿè£…ã®**å„ªå…ˆé †ä½ä»˜ã‘**ã¨**å·¥æ•°è¦‹ç©ã‚‚ã‚Š**ã®ãŸã‚ã®è³‡æ–™ã§ã™ã€‚å…¨ã¦ã®é …ç›®ã‚’å®Ÿè£…ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚Criticalï¼ˆPriority 1ï¼‰ã®8é …ç›®ã‚’å®Ÿè£…ã™ã‚Œã°ã€å®Ÿç”¨ä¸Šååˆ†ãªä¿¡é ¼æ€§ãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚
